❤️背景简介❤️

    IT作为今天最火热的行业之一，其中又衍生机器学习，深度学习，数据挖掘，等等。
    这些学科能够发展，离不开数据总量快速的增长，以及现在电脑对信息的快速处理。
    显然，爬虫已经逐渐成为了今天的程序员或者计算机爱好者的被动技能。 

    网络爬虫（英语：web crawler），也叫网络蜘蛛（spider），
        是一种根据一定规则来自动浏览万维网的网络机器人。
        其目的一般为编纂网络索引,也是搜索引擎最基本的原理。
        据说google 最初就是用python写了网页爬虫. 

    作为自身使用来说，没有必要去做一个完善的自动化爬虫，
    只需要根据自身的需求，量身定制一个最适合我们需求的，能够抓取我们想要的信息就可以了。 

    爬虫的学习路线是陡峭的。入门很容易，随手google一下就可以找到一堆抓取静态网页的代码。
    这的确可以说是做出了爬虫，但是离我们真正的目标差了很远。
    所以先列出必学的基本知识.
      * 一门编程语言
      * 网页前端知识
      * 能够使用常用数据库
      * 计算机网络协议
      * 了解一些常用的前端加密算法（如果你做产品抓商业化的信息，那这是不可少的）
      * 熟练的运用常用的工具，抓包，模拟发送请求等等
      * 还有一些细节性的东西（验证码，ip地址等等）

      掌握了以上这些，就可以应付绝大部分的任务了。
      知识没有捷径，如果仅仅是玩玩的话，几天过去就没了。
      想学习的话，这些东西跳不过去，要花时间精力慢慢掌握。


    得到了信息之后，自然要做些什么。
    除了商业用处之外，也可以有很多其他的“玩法”。
        抓取知乎数据分析知乎用户并做出图谱。
        抓取网页云音乐评论做自然语言处理得到人们对不同歌手的评价。
        抓取豆瓣电影按评分挑出自己喜欢看, and so on.
    当然，价值最高的还是用来做商业分析，这也催生了爬虫工程师这个职业。






爬虫:   
    自动获取网互联网信息的程序或脚本,提取网页中有价值的数据.
    - 可以爬某个网站下的某些数据.
    - 也可以爬全世界所有网站下的某些数据.

互联网: 
    无数网页组成.
    网页使用 URL  网址访问. 
    网页使用 HTTP 协议传输.
    网页使用 HTML 语言编写.

互联网就像一张超级巨大的蜘蛛网. 爬虫就是里面的一中小蜘蛛.
每个网页都有很多链接(URL).理论上通过这些链接可以通到世界上任意一个网站.
爬虫日夜不停的去查看所有网页.遇到有价值的东西就下载下来.




❤️爬虫作用:❤️
  ❗️搜索引擎的基础!❗️
    你写了一篇文章.发表到网络上. 谷歌就能搜索到你的文章! 为什么?!
    谷歌有个数据库. 哪篇文章 的关键词是什么.
    互联网这么大,靠人工维护这个数据库是不现实的,
    只有爬虫才能收录网页. 只有你的网页被收录了 你的网页才会出现在搜索结果中

你要研究SEO. 就得知道爬虫.

做爬虫去 收集大类的代理服务器!!! 不同的服务器就是不同的IP.
这些就可以刷票了!!!!

免费的东西总是不稳定的. 定期更新IP地址, 删除无效的服务器.


可以抓twitter 的说说.. 每天几亿条数据.
下下来要存到哪里呢...
公司可以用集群啊. hadoop. spark... 
穷人只能自己台式机组raid.

储存数据肯定用数据库了. mysql.
几十亿的数据. 必须大量优化啊. 不然查一条都要半天...
要可以几秒内 读取到出某条数据 才行...


Python 是神器. 就因为谷歌用这个 我就绝对学python.而不是php perl..
Python 很简单!!! 没编程的也能入门.



获取某网站所有用户的头像.
更具头像被点击的次数.来预测出最受欢迎的头像


抓取 facebook 上的 sleep关键词. 来判断出大家的睡眠时间..
因为看来很多人喜欢在睡前会说一声我睡了。




    方便的获取数据!!!
    喜欢看美女图片? 写个爬虫把某网站所有的图片都下下来 慢慢看.
    喜欢看电影?     写个爬虫把某网站所有种子都下下来,慢慢下载.
    喜欢某个妹子?   写个爬虫把她所有说说都下下来研究
    喜欢秒杀超值物品 写个爬虫.... 还是脚本 ??

天气数据.
网站用户数据.
现在这个年代. 数据才是最重要的啊!!!

比价网站 都是爬虫.
电影推荐 也是爬虫..

扫描器 好像也是爬虫....
一般都要先爬去 数据. 再处理数据...






爬虫实现原理:
  抓取网页, 
    储存网页, 
      分析网页. 
        显示结果; 
          抓取下一个网页. 储存,分析, 重复循环.


抓取网页:
  平时上网,浏览器中输入网址,按下回车. 显示网页内容.
    其实就是 发送请求 + 接收请求
      发送请求里面 有要访问的网址
      接收请求里面 有要显示的内容
        浏览器都可以上网.更别说用终端了.




爬虫储备知识:
  下载网页:urllib urllib2
  解析网页: BeautifulSoup，熟悉JQuery的可以用Pyquery
  提交请求.使用Requests来提交各种类型的请求，支持重定向，cookies等。
.使用Selenium，模拟浏览器提交类似用户的操作，处理js动态产生的网页



❤️爬取数据❤️


URL 解析器:  
    管理要抓取的URL  和 已经抓取过的URL.

HTML 下载器: urllib2 , requests
    下载数据 保存成字符串.

解析器: beautifulsoup
    一方面抓取有价值的数据. 一方面把新的URL补充到 URL管理器

输出器













# 网站会对 request header 做一些限制，
# 不是浏览器 不回复你, 就是为了反爬虫...
# 就需要设置一个 User-Agent 来骗对方的服务器

# http 请求状态有很多种，大多数情况下 200 是我们理想中的状态，
# 我们用 requests 的一个 status_code 方法来判断是否访问成功，
# 然后使用 print 一下，相当于打印日志了，
# 当我们做大规模抓取的时候，print 显然不是一个很好的选择，
# Python 的 logging 模块能够更好的胜任这项工作，
# 为了方便起见，我们暂时使用简单直接粗暴的 print 来做代替 logging 的工作

# 由于网络原因，可能会出现访问超时的问题，
# 为了让代码更健壮，我们可以设置 timeout, 
# 同时，如果确定网页不需要重定向， 可以设置 allow_redirects=False

# 至此，我们完成了一个网页的抓取，并且做了相关的异常处理。



import requests

def get_data(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.87 Safari/537.36"
    }
    try:
        html = requests.get(url, headers=headers, allow_redirects=Fasle, timeout=3)
        if html.status_code == 200:
            print(url, '@ok200', str(time.ctime()))
            with open('content.html', 'wb') as fw:
                fw.write(html.content)
                fw.close()
        else:
            print(url, 'wrong', str(time.ctime()))
    except Exception as e:
        print(url, e, str(time.ctime()))

if __name__ == '__main__':
    url = "Example Domain"
    get_data(url)













    # 网页抓下来后 需要分析网页.
  # 分析网页可以用正则式. 也可以用 beautifulsoup


# 导入正则式模块 >>> import re

常用的是  
贪婪匹配 和 非贪婪匹配.
单行匹配 和 全文匹配.

text = """Sxchaoinfo@EgithubE
    xchaoinfo@wechat
    xchaoinfo@zhihuE"""

# re.S 全文匹配
pa = r'S.*?E'
re.findall(pa, text) -> 返回的是["Sxchaoinfo@E",]
re.findall(pa, text, re.S) -> 返回的是["Sxchaoinfo@E",]





